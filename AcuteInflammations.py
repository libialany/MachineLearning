# video: LibProjectAlpha
# -*- coding: utf-8 -*-
"""Acute_Inflammations_Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B2KeqYOw2h0EUtAiIaV4LkyMaz4I4scw

# <center><b>Acute Inflammations Analysis with Decision Trees</b></center>

We will analyse the Acute Inflammations dataset using Decision Trees algorithm to understand and perform the presumptive diagnosis of two diseases of the urinary system

## 1. Decision Trees as Interpretable Models

Import packages
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree
from sklearn.tree import _tree
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score, confusion_matrix, roc_curve, auc, accuracy_score,classification_report
import warnings
warnings.filterwarnings('ignore')

print("========== Packages Imported Successfully ==========\n")

"""### (a) Obtain Data

Get the Accute Inamations Data Set

Download the Accute Inflamations data from https://archive.ics.uci.edu/ml/datasets/Acute+Inflammations
"""

# Defining column headers for the diagnosis data.
!wget ""
"""
Column references:
a1 = attribute-1 = Temperature of patient (35C-42C)
a2 = attribute-2 = Occurrence of nausea (yes, no)
a3 = attribute-3 = Lumbar pain (yes, no)
a4 = attribute-4 = Urine pushing or continuous need for urination (yes, no)
a5 = attribute-5 = Micturition pains (yes, no)
a6 = attribute-6 = Burning of urethra, itch, swelling of urethra outlet (yes, no)
d1 = decision-1 = Inflammation of urinary bladder (yes, no)
d2 = decision-2 = Nephritis of renal pelvis origin (yes, no)
"""

def get_column_headers():
    headers = ["a1", "a2", "a3", "a4", "a5", "a6", "d1", "d2"]
    return headers

# Obtaining data.

data_path = "diagnosis.data"
data_df = pd.read_csv(data_path, encoding='UTF_16', header=None, sep="\s+", decimal=",")
data_df.columns = get_column_headers()
print("Data shape:", data_df.shape)
data_df.head()

"""### (b) Build a decision tree

Build a decision tree on the whole data set and plot it.
"""

# Converting categorical data to numeric type.
def convert_categorical_to_numeric(df):
    for col in get_column_headers():
        df.loc[df[col] == "no", col] = 0
        df.loc[df[col] == "yes", col] = 1
    return df

# Function to get the X and Y for data.
def get_X_Y_data(df, label_count=1):
    if label_count != 1:
        # Block execution if label_count = 2.
        df_X = df.iloc[: , :-2]
        df_Y1 = df.iloc[: , -2:-1]
        df_Y2 = df.iloc[: , -1:]
        df_Y1 = df_Y1.astype("int64")
        df_Y2 = df_Y2.astype("int64")
        return df_X, df_Y1, df_Y2
    else:
        # Default block.
        df_X = df.iloc[: , 0:6]
        df_Y = df.iloc[: , 6:]
        df_Y = df_Y.astype("int64")
        return df_X, df_Y

data_df = convert_categorical_to_numeric(data_df)
print("Data shape:", data_df.shape)
data_df.head()

train_X, train_Y1, train_Y2 = get_X_Y_data(data_df, label_count=2)
print("Train_X shape: {}\nTrain_Y1 shape: {}\nTrain_Y2 shape: {} \n".format(train_X.shape, train_Y1.shape, train_Y2.shape))

# Function to build the decision tree visualization.
def get_decisiontree_text_and_plot(data_X, data_Y):
    dt_clf = DecisionTreeClassifier(random_state = 50)
    dt_clf.fit(data_X, data_Y)

    text = tree.export_text(dt_clf, feature_names=list(data_X.columns))
    print("\n========== Decision Tree Text ==========\n")
    print(text)

    print("\n========== Decision Tree Plot ==========\n")
    plot = tree.plot_tree(dt_clf, filled=True, feature_names=data_X.columns, class_names=["0", "1"], rounded=True, fontsize=8)
    return dt_clf

# Decision tree for d1.
get_decisiontree_text_and_plot(train_X, train_Y1)

# Decision tree for d2.
get_decisiontree_text_and_plot(train_X, train_Y2)

# Decision tree for both, d1 and d2.

train_X, train_Y = get_X_Y_data(data_df)
print("Train_X shape: {}\nTrain_Y shape: {}\n".format(train_X.shape, train_Y.shape))

dt_clf = get_decisiontree_text_and_plot(train_X, train_Y)

"""### (c) Cost-Complexity Pruning

Use cost-complexity pruning to find a minimal decision tree with high interpretability.
"""

# Spliting the data into train-test as 80-20.
X_train, X_test, Y_train, Y_test = train_test_split(train_X, train_Y, random_state=50, test_size=0.2)

# Function to get the best model based on pruning parameters.
def get_best_model_with_GCV():
    parameters = {"max_depth": [i for i in range(5)], "ccp_alpha": [0.00, 0.01, 0.02, 0.03, 0.04, 0.05]}
    dt_clf = GridSearchCV(DecisionTreeClassifier(random_state=50), parameters)
    dt_clf.fit(X_train, Y_train)
    result = {"best_score": round(dt_clf.best_score_, 4), "best_params": dt_clf.best_params_, "accurate_model": dt_clf.best_estimator_}
    return result

# Displaying the results.
best_result = get_best_model_with_GCV()
best_score = best_result["best_score"]
best_params = best_result["best_params"]
accurate_model = best_result["accurate_model"]

print("Best Score is {}".format(best_score))
print("Best Params are {}".format(best_params))

# Fitting the best obtained model to train and test data.
best_clf = accurate_model
best_clf.fit(X_train, Y_train)

Y_train_pred = best_clf.predict(X_train)
Y_test_pred = best_clf.predict(X_test)

print("Train Accuracy score =", accuracy_score(Y_train, Y_train_pred))
print("Test Accuracy score =", accuracy_score(Y_test, Y_test_pred))

features = ["temperature", "nausea_occurence", "lumbar_pain", "urine_pushing", "micturition_pains", "urethra_burning"]
labels = ["inflamation", "nephtitis"]

text = tree.export_text(best_clf, feature_names=features)
print("\n========== Pruned Decision Tree Text ==========\n")
print(text)
print("\n========== Pruned Decision Tree Plot ==========\n")
plot = tree.plot_tree(best_clf, filled=True, feature_names=features, class_names=labels, rounded=True, fontsize=6)

"""<b>Justification:</b>
<br>
- The above model has been build using the best ccp_alpha and max_depth values obtained from grid search, and gives the train as well as test accuracies as 1.
- Hence, this is the best minimal decision tree obtained using cost-complexity pruning.
"""









"""<b>High Interpretability:</b>
<br>
We know that, applying GridSearchCV with paramenters of ccp_alpha and max_depth gave us an optimum model with ccp_alpha as 0, and max_depth of 4. However, max_depth of 4 means there is no significant change in the tree from the original condition. Hence, to make the tree even more interpretable, we will just use max_depth as 1.
"""

# Fitting the best obtained model to train and test data.
hi_clf = DecisionTreeClassifier(max_depth=1, random_state=50, ccp_alpha=0)
hi_clf.fit(X_train, Y_train)

Y_train_pred = hi_clf.predict(X_train)
Y_test_pred = hi_clf.predict(X_test)

print("Train Accuracy score =", accuracy_score(Y_train, Y_train_pred))
print("Test Accuracy score =", accuracy_score(Y_test, Y_test_pred))

features = ["temperature", "nausea_occurence", "lumbar_pain", "urine_pushing", "micturition_pains", "urethra_burning"]
labels = ["inflamation", "nephtitis"]

text = tree.export_text(hi_clf, feature_names=features)
print("\n========== Pruned Decision Tree Text | High Interpretability ==========\n")
print(text)
print("\n========== Pruned Decision Tree Plot | High Interpretability ==========\n")
plot = tree.plot_tree(hi_clf, filled=True, feature_names=features, class_names=labels, rounded=True, fontsize=9)

"""<b>References:</b>
<br>
1) https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html
2) https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html
3) https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html
4) https://www.kdnuggets.com/2017/05/simplifying-decision-tree-interpretation-decision-rules-python.html
5) https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html
"""
